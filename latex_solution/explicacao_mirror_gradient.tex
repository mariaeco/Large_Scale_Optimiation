\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}

\geometry{a4paper, margin=2.5cm}

\title{Algoritmo do Gradiente Espelhado (Mirror Descent)}
\author{Explicação do Algoritmo Implementado}
\date{\today}

\begin{document}

\maketitle

\section{Introdução}

O \textbf{Gradiente Espelhado} (também conhecido como \textbf{Mirror Descent}) é um algoritmo de otimização que generaliza o método do gradiente descendente clássico. Ele é particularmente útil para problemas de otimização em espaços não-euclidianos e quando queremos incorporar informações sobre a geometria do problema através de uma função de distância.

\section{Conceitos Fundamentais}

\subsection{Divergência de Bregman}

O algoritmo do gradiente espelhado baseia-se na \textbf{divergência de Bregman}, que é uma generalização da distância euclidiana. Para uma função convexa $\phi$, a divergência de Bregman é definida como:

\begin{equation}
D_\phi(x, y) = \phi(x) - \phi(y) - \langle \nabla \phi(y), x - y \rangle
\end{equation}

onde $\nabla \phi(y)$ é o gradiente de $\phi$ no ponto $y$.

\subsection{Função de Distância}

No nosso algoritmo, utilizamos a \textbf{função euclidiana}:
\begin{align}
\phi(x) &= \frac{1}{2}\|x\|^2 = \frac{1}{2}\sum_{i=1}^n x_i^2 \\
\nabla \phi(x) &= x
\end{align}

Para esta função, a divergência de Bregman se reduz à distância euclidiana ao quadrado:
\begin{equation}
D_\phi(x, y) = \frac{1}{2}\|x - y\|^2
\end{equation}

\section{Algoritmo do Gradiente Espelhado}

\subsection{Formulação do Problema}

Dado um problema de minimização:
\begin{equation}
\min_{x \in \mathbb{R}^n} f(x)
\end{equation}

onde $f$ é uma função diferenciável.

\subsection{Algoritmo}

O algoritmo do gradiente espelhado funciona da seguinte forma:

\begin{enumerate}[label=\textbf{Passo \arabic*:}]
    \item \textbf{Inicialização:} Escolha um ponto inicial $x_0$ e parâmetros $\eta > 0$ (taxa de aprendizado) e tolerância $\epsilon$.
    
    \item \textbf{Iteração $k$:}
    \begin{enumerate}[label=(\alph*)]
        \item Calcule o gradiente: $g_k = \nabla f(x_k)$
        \item Resolva o subproblema:
        \begin{equation}
        x_{k+1} = \arg\min_{x} \left\{ \langle g_k, x \rangle + \frac{1}{\eta} D_\phi(x, x_k) \right\}
        \end{equation}
        \item Verifique convergência: se $\|x_{k+1} - x_k\| < \epsilon$, pare.
    \end{enumerate}
\end{enumerate}

\subsection{Resolução do Subproblema}

Para a função euclidiana $\phi(x) = \frac{1}{2}\|x\|^2$, o subproblema se torna:

\begin{align}
x_{k+1} &= \arg\min_{x} \left\{ \langle g_k, x \rangle + \frac{1}{2\eta}\|x - x_k\|^2 \right\} \\
&= \arg\min_{x} \left\{ \langle g_k, x \rangle + \frac{1}{2\eta}\|x\|^2 - \frac{1}{\eta}\langle x_k, x \rangle + \frac{1}{2\eta}\|x_k\|^2 \right\}
\end{align}

Tomando o gradiente e igualando a zero:
\begin{equation}
g_k + \frac{1}{\eta}(x_{k+1} - x_k) = 0
\end{equation}

Portanto:
\begin{equation}
x_{k+1} = x_k - \eta g_k
\end{equation}

\section{Implementação no Código}

\subsection{Estrutura Principal}

O algoritmo implementado segue a seguinte estrutura:

\begin{verbatim}
def gradiente_espelhado(f, x0, eta, max_iter, tol, phi, phi_grad):
    x = x0
    fo = f(x0)  # Valor inicial da função
    
    for k in range(max_iter):
        # 1. Calcular gradiente
        grad_f = calcular_gradiente(f, x)
        
        # 2. Resolver subproblema
        x_novo, f_novo = resolver_subproblema(grad_f, x, eta, phi, phi_grad)
        
        # 3. Verificar convergência
        if ||x_novo - x|| < tol:
            break
            
        x = x_novo
        fo = f(x)
    
    return x, fo, k
\end{verbatim}

\subsection{Cálculo do Gradiente}

O gradiente é calculado numericamente usando diferenças finitas centrais:

\begin{verbatim}
def calcular_gradiente(f, x):
    n = len(x)
    grad = zeros(n)
    
    for i in range(n):
        h = 1e-6
        x_plus = x.copy()
        x_plus[i] += h
        x_minus = x.copy()
        x_minus[i] -= h
        
        grad[i] = (f(x_plus) - f(x_minus)) / (2 * h)
    
    return grad
\end{verbatim}

\subsection{Resolução do Subproblema}

O subproblema é resolvido usando o otimizador BFGS do SciPy:

\begin{verbatim}
def resolver_subproblema(grad_f, x_atual, eta, phi, phi_grad):
    def funcao_subproblema(x):
        bregman_div = phi(x) - phi(x_atual) - dot(phi_grad(x_atual), x - x_atual)
        return dot(grad_f, x) + (1/eta) * bregman_div
    
    resultado = minimize(funcao_subproblema, x0=x_atual)
    return resultado.x, resultado.fun
\end{verbatim}

\section{Vantagens do Gradiente Espelhado}

\begin{itemize}
    \item \textbf{Generalização:} Funciona em espaços não-euclidianos
    \item \textbf{Flexibilidade:} Permite incorporar informações geométricas através da função $\phi$
    \item \textbf{Convergência:} Garante convergência para funções convexas
    \item \textbf{Estabilidade:} Mais robusto que o gradiente descendente clássico em alguns casos
\end{itemize}

\section{Parâmetros do Algoritmo}

No código implementado, os parâmetros utilizados são:

\begin{itemize}
    \item \textbf{Taxa de aprendizado ($\eta$):} 0.01 (padrão)
    \item \textbf{Máximo de iterações:} 1000 (padrão)
    \item \textbf{Tolerância:} $10^{-6}$ (padrão)
    \item \textbf{Precisão do gradiente:} $h = 10^{-6}$ (diferenças finitas)
\end{itemize}

\section{Problemas Testados}

O algoritmo foi testado em 16 problemas de otimização da coleção Liu-Nocedal:

\begin{enumerate}
    \item ROSENBROCK
    \item PENALTY
    \item TRIGONOMETRIC
    \item EXTENDED\_ROSENBROCK
    \item EXTENDED\_POWELL
    \item QOR
    \item GOR
    \item PSP
    \item TRIDIAGONAL
    \item ENGGVAL1
    \item LINEAR\_MINIMUM\_SURFACE
    \item SQUARE\_ROOT\_1
    \item SQUARE\_ROOT\_2
    \item FREUDENTHAL\_ROTH
    \item SPARSE\_MATRIX\_SQRT
    \item ULTS0
\end{enumerate}

\section{Resultados e Análise}

O algoritmo gera três tabelas principais:

\begin{enumerate}
    \item \textbf{Tabela de Problemas:} Lista todos os problemas e o número de variáveis
    \item \textbf{Tabela de Convergência:} Mostra iterações, valor mínimo encontrado e precisão
    \item \textbf{Tabela de Soluções:} Apresenta as primeiras 5 variáveis da solução encontrada
\end{enumerate}

\section{Conclusão}

O algoritmo do gradiente espelhado implementado oferece uma abordagem robusta e flexível para resolver problemas de otimização não-linear. Sua principal vantagem é a capacidade de incorporar informações geométricas através da função de distância, tornando-o adequado para uma ampla gama de problemas de otimização.

A implementação utiliza técnicas numéricas estáveis e inclui tratamento de erros, garantindo robustez na resolução dos problemas testados.

\end{document}
