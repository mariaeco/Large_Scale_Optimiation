\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}

\geometry{a4paper, margin=2.5cm}

\title{Algoritmo da Descida por Coordenadas (Coordinate Descent)}
\author{Explicação do Algoritmo Implementado}
\date{\today}

\begin{document}

\maketitle

\section{Introdução}

A \textbf{Descida por Coordenadas} (também conhecida como \textbf{Coordinate Descent}) é um algoritmo de otimização que minimiza uma função objetivo multivariável minimizando sequencialmente em relação a uma coordenada por vez, mantendo todas as outras coordenadas fixas. Este método é particularmente eficaz para problemas onde a minimização unidimensional é computacionalmente mais simples que a minimização multidimensional.

\section{Conceitos Fundamentais}

\subsection{Ideia Principal}

O algoritmo da descida por coordenadas baseia-se na seguinte estratégia:

\begin{enumerate}
    \item Dado um ponto atual $x^{(k)}$, escolha uma coordenada $i$
    \item Minimize $f(x)$ em relação à coordenada $x_i$, mantendo todas as outras coordenadas fixas
    \item Atualize apenas a coordenada $i$ com o valor ótimo encontrado
    \item Repita o processo para a próxima coordenada
\end{enumerate}

\subsection{Formulação Matemática}

Para um problema de minimização:
\begin{equation}
\min_{x \in \mathbb{R}^n} f(x)
\end{equation}

O algoritmo da descida por coordenadas funciona da seguinte forma:

\begin{enumerate}
    \item \textbf{Inicialização:} Escolha um ponto inicial $x^{(0)}$
    \item \textbf{Iteração $k$:} Para cada coordenada $i = 1, 2, \ldots, n$:
    \begin{equation}
    x_i^{(k+1)} = \arg\min_{x_i} f(x_1^{(k)}, \ldots, x_{i-1}^{(k)}, x_i, x_{i+1}^{(k)}, \ldots, x_n^{(k)})
    \end{equation}
    \item \textbf{Convergência:} Pare quando $\|x^{(k+1)} - x^{(k)}\| < \epsilon$
\end{enumerate}

\section{Algoritmo da Descida por Coordenadas}

\subsection{Algoritmo Básico}

\begin{enumerate}[label=\textbf{Passo \arabic*:}]
    \item \textbf{Inicialização:} Escolha um ponto inicial $x_0$ e tolerância $\epsilon$.
    
    \item \textbf{Iteração $k$:}
    \begin{enumerate}[label=(\alph*)]
        \item Para cada coordenada $i = 1, 2, \ldots, n$:
        \begin{enumerate}[label=\roman*]
            \item Defina a função unidimensional: $g_i(t) = f(x_1, \ldots, x_{i-1}, t, x_{i+1}, \ldots, x_n)$
            \item Resolva: $t^* = \arg\min_t g_i(t)$
            \item Atualize: $x_i = t^*$
        \end{enumerate}
        \item Verifique convergência: se $\|x^{(k+1)} - x^{(k)}\| < \epsilon$, pare.
    \end{enumerate}
\end{enumerate}

\subsection{Minimização Unidimensional}

Para cada coordenada, precisamos resolver um problema de minimização unidimensional. No nosso algoritmo, utilizamos o método de Brent:

\begin{equation}
t^* = \arg\min_{t \in \mathbb{R}} g_i(t)
\end{equation}

onde $g_i(t) = f(x_1, \ldots, x_{i-1}, t, x_{i+1}, \ldots, x_n)$.

\section{Implementação no Código}

\subsection{Estrutura Principal}

O algoritmo implementado segue a seguinte estrutura:

\begin{verbatim}
def descida_por_coordenadas(f, x0, max_iter, tol):
    x = x0
    n = len(x)
    fo = f(x0)  # Valor inicial da função
    
    for k in range(max_iter):
        x_anterior = x.copy()
        
        # Atualizar cada coordenada sequencialmente
        for i in range(n):
            x[i] = minimizar_coordenada(f, x, i)
        
        # Verificar convergência
        if ||x - x_anterior|| < tol:
            break
            
        fo = f(x)
    
    return x, fo, k
\end{verbatim}

\subsection{Minimização de Coordenada}

Para cada coordenada, o algoritmo resolve um problema unidimensional:

\begin{verbatim}
def minimizar_coordenada(f, x, i):
    def funcao_1d(xi):
        x_temp = x.copy()
        x_temp[i] = xi
        return f(x_temp)
    
    resultado = minimize_1d(funcao_1d, x0=x[i])
    return resultado.x
\end{verbatim}

\subsection{Métodos de Minimização 1D}

O algoritmo implementa três métodos para minimização unidimensional:

\begin{enumerate}
    \item \textbf{Método de Brent:} Busca linear eficiente
    \item \textbf{Método de Newton 1D:} Usa primeira e segunda derivadas
    \item \textbf{Fallback:} Método de Brent como padrão
\end{enumerate}

\subsubsection{Método de Newton 1D}

Para o método de Newton, utilizamos:

\begin{align}
x_{k+1} &= x_k - \frac{f'(x_k)}{f''(x_k)} \\
f'(x) &\approx \frac{f(x+h) - f(x-h)}{2h} \\
f''(x) &\approx \frac{f(x+h) - 2f(x) + f(x-h)}{h^2}
\end{align}

\section{Vantagens da Descida por Coordenadas}

\begin{itemize}
    \item \textbf{Simplicidade:} Cada iteração resolve apenas problemas unidimensionais
    \item \textbf{Eficiência:} Pode ser mais rápido que métodos de ordem superior para problemas específicos
    \item \textbf{Paralelização:} Coordenadas podem ser atualizadas em paralelo (em algumas versões)
    \item \textbf{Memória:} Requer menos memória que métodos que armazenam gradientes completos
    \item \textbf{Robustez:} Menos sensível a problemas de condicionamento
\end{itemize}

\section{Desvantagens e Limitações}

\begin{itemize}
    \item \textbf{Convergência lenta:} Pode convergir lentamente para problemas mal condicionados
    \item \textbf{Dependência da ordem:} A ordem de atualização das coordenadas pode afetar a convergência
    \item \textbf{Problemas não separáveis:} Menos eficaz para funções com forte acoplamento entre variáveis
    \item \textbf{Mínimos locais:} Pode ficar preso em mínimos locais
\end{itemize}

\section{Parâmetros do Algoritmo}

No código implementado, os parâmetros utilizados são:

\begin{itemize}
    \item \textbf{Máximo de iterações:} 1000 (padrão)
    \item \textbf{Tolerância:} $10^{-6}$ (padrão)
    \item \textbf{Precisão das derivadas:} $h = 10^{-6}$ (diferenças finitas)
    \item \textbf{Método 1D:} Brent (padrão)
\end{itemize}

\section{Problemas Testados}

O algoritmo foi testado em 16 problemas de otimização da coleção Liu-Nocedal:

\begin{enumerate}
    \item ROSENBROCK
    \item PENALTY
    \item TRIGONOMETRIC
    \item EXTENDED\_ROSENBROCK
    \item EXTENDED\_POWELL
    \item QOR
    \item GOR
    \item PSP
    \item TRIDIAGONAL
    \item ENGGVAL1
    \item LINEAR\_MINIMUM\_SURFACE
    \item SQUARE\_ROOT\_1
    \item SQUARE\_ROOT\_2
    \item FREUDENTHAL\_ROTH
    \item SPARSE\_MATRIX\_SQRT
    \item ULTS0
\end{enumerate}

\section{Resultados e Análise}

O algoritmo gera três tabelas principais:

\begin{enumerate}
    \item \textbf{Tabela de Problemas:} Lista todos os problemas e o número de variáveis
    \item \textbf{Tabela de Convergência:} Mostra iterações, valor mínimo encontrado e precisão
    \item \textbf{Tabela de Soluções:} Apresenta as primeiras 5 variáveis da solução encontrada
\end{enumerate}

\section{Convergência}

\subsection{Condições de Convergência}

A descida por coordenadas converge para um ponto estacionário sob as seguintes condições:

\begin{enumerate}
    \item A função objetivo $f$ é contínua e diferenciável
    \item O conjunto de níveis $\{x : f(x) \leq f(x_0)\}$ é compacto
    \item A sequência de pontos gerada é limitada
\end{enumerate}

\subsection{Taxa de Convergência}

A taxa de convergência da descida por coordenadas depende da estrutura do problema:

\begin{itemize}
    \item \textbf{Funções separáveis:} Convergência linear rápida
    \item \textbf{Funções mal condicionadas:} Convergência lenta
    \item \textbf{Funções bem condicionadas:} Convergência linear moderada
\end{itemize}

\section{Exemplo Prático}

\subsection{Problema de Exemplo}

Considere o problema de minimizar:
\begin{equation}
f(x_1, x_2) = (x_1 - 1)^2 + 100(x_2 - x_1^2)^2
\end{equation}

\subsection{Execução do Algoritmo}

\begin{enumerate}
    \item \textbf{Inicialização:} $x^{(0)} = (0, 0)$
    \item \textbf{Iteração 1:}
    \begin{itemize}
        \item Minimizar em $x_1$: $g_1(t) = (t-1)^2 + 100(0-t^2)^2$
        \item Minimizar em $x_2$: $g_2(t) = (x_1-1)^2 + 100(t-x_1^2)^2$
    \end{itemize}
    \item Repetir até convergência
\end{enumerate}

\section{Implementação da Classe}

\subsection{CoordinateDescentSolver}

A classe implementada inclui:

\begin{itemize}
    \item \textbf{Configuração de problemas:} Todos os 16 problemas de Liu-Nocedal
    \item \textbf{Resolução individual:} Método \texttt{solve\_problem()}
    \item \textbf{Resolução em lote:} Método \texttt{solve\_all\_problems()}
    \item \textbf{Geração de LaTeX:} Método \texttt{generate\_latex\_table()}
    \item \textbf{Resumo de resultados:} Método \texttt{print\_summary()}
\end{itemize}

\subsection{Tratamento de Erros}

O algoritmo inclui tratamento robusto de erros:

\begin{itemize}
    \item Captura de exceções durante a otimização
    \item Fallback para métodos alternativos
    \item Relatórios detalhados de falhas
    \item Medição de tempo de execução
\end{itemize}

\section{Conclusão}

O algoritmo da descida por coordenadas implementado oferece uma abordagem simples e eficaz para resolver problemas de otimização não-linear. Sua principal vantagem é a simplicidade de implementação e a capacidade de resolver problemas unidimensionais de forma eficiente.

A implementação utiliza métodos numéricos estáveis para minimização unidimensional e inclui tratamento de erros, garantindo robustez na resolução dos problemas testados. Embora possa convergir mais lentamente que métodos de ordem superior para alguns problemas, sua simplicidade e eficiência o tornam uma ferramenta valiosa para uma ampla gama de problemas de otimização.

O algoritmo é particularmente adequado para:
\begin{itemize}
    \item Problemas com funções objetivo separáveis
    \item Problemas de grande escala onde métodos de ordem superior são computacionalmente caros
    \item Aplicações onde a simplicidade de implementação é prioritária
    \item Problemas onde a paralelização por coordenadas é desejável
\end{itemize}

\end{document}