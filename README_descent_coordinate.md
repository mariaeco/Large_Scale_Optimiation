# Descida por Coordenadas Otimizada

Este arquivo implementa o algoritmo de **Descida por Coordenadas** otimizado para problemas de otimizaÃ§Ã£o nÃ£o-linear de larga escala, baseado na coleÃ§Ã£o de problemas de Liu e Nocedal (1989).

## ğŸ“‹ **FormulaÃ§Ã£o MatemÃ¡tica**

### **Problema de OtimizaÃ§Ã£o**
Considere o problema irrestrito:
```
min f(x)
x âˆˆ â„â¿
```

onde `f: â„â¿ â†’ â„` Ã© uma funÃ§Ã£o diferenciÃ¡vel.

### **Algoritmo de Descida por Coordenadas**

O algoritmo resolve iterativamente subproblemas de dimensÃ£o reduzida:

```
Para k = 0, 1, 2, ...:
  1. Selecionar bloco de coordenadas Iâ‚– âŠ† {1, 2, ..., n}
  2. Resolver: x^{k+1} = argmin_{x_{Iâ‚–}} f(x_{Iâ‚–}, x^k_{Â¬Iâ‚–})
  3. Verificar convergÃªncia: ||x^{k+1} - x^k|| < Îµ
```

### **SeleÃ§Ã£o de Coordenadas**
- **EstratÃ©gia**: SeleÃ§Ã£o aleatÃ³ria de blocos
- **Tamanho do bloco**: ConfigurÃ¡vel (padrÃ£o: 10)
- **Garantia**: Todas as coordenadas sÃ£o eventualmente selecionadas

## ğŸ”§ **FunÃ§Ãµes Implementadas**

### **1. `calcular_gradiente(f, x)`**
```python
def calcular_gradiente(f, x):
    """
    Calcula gradiente numericamente usando diferenÃ§as finitas centrais
    
    Args:
        f: funÃ§Ã£o objetivo
        x: ponto atual
    
    Returns:
        grad: vetor gradiente
    """
```
**FÃ³rmula**: `âˆ‡f_i â‰ˆ [f(x + hÂ·e_i) - f(x - hÂ·e_i)] / (2h)`

### **2. `descida_por_coordenadas_otimizada(f, x0, max_iter, tol, block_size=10)`**
```python
def descida_por_coordenadas_otimizada(f, x0, max_iter, tol, block_size=10):
    """
    Algoritmo principal de descida por coordenadas
    
    Args:
        f: funÃ§Ã£o objetivo
        x0: ponto inicial
        max_iter: mÃ¡ximo de iteraÃ§Ãµes
        tol: tolerÃ¢ncia para convergÃªncia
        block_size: tamanho do bloco de coordenadas
    
    Returns:
        x: ponto Ã³timo
        fo: valor da funÃ§Ã£o objetivo
        k: nÃºmero de iteraÃ§Ãµes
    """
```

**CaracterÃ­sticas**:
- SeleÃ§Ã£o aleatÃ³ria de blocos de coordenadas
- Garantia de cobertura completa das coordenadas
- CritÃ©rio de convergÃªncia baseado na mudanÃ§a do ponto

### **3. `minimizar_bloco_coordenadas(f, x, indices)`**
```python
def minimizar_bloco_coordenadas(f, x, indices):
    """
    Minimiza f(x) em relaÃ§Ã£o a um bloco de coordenadas
    
    Args:
        f: funÃ§Ã£o objetivo
        x: ponto atual
        indices: lista de Ã­ndices das coordenadas
    
    Returns:
        x: ponto atualizado
    """
```

**Algoritmo**:
```
Se |indices| = 1:
    x[i] = minimizar_coordenada(f, x, i)
SenÃ£o se |indices| â‰¤ 3:
    x[indices] = minimize(f_bloco, x0_bloco, method='Nelder-Mead')
SenÃ£o:
    Para cada i em indices:
        x[i] = minimizar_coordenada(f, x, i)
```

### **5. `minimizar_coordenada(f, x, i)`**
```python
def minimizar_coordenada(f, x, i):
    """
    Minimiza f(x) em relaÃ§Ã£o Ã  coordenada i
    
    Args:
        f: funÃ§Ã£o objetivo
        x: ponto atual
        i: Ã­ndice da coordenada
    
    Returns:
        x_i: valor otimizado da coordenada i
    """
```

**ImplementaÃ§Ã£o**:
- Define funÃ§Ã£o 1D: `g(x_i) = f(x_1, ..., x_i, ..., x_n)`
- Usa `minimize_1d()` com mÃ©todo de Brent

### **6. `minimize_1d(f, x0)`**
```python
def minimize_1d(f, x0):
    """
    MinimizaÃ§Ã£o 1D usando mÃ©todo de Brent
    
    Args:
        f: funÃ§Ã£o 1D
        x0: ponto inicial
    
    Returns:
        resultado: objeto com atributo .x
    """
```

**MÃ©todo**: `scipy.optimize.minimize_scalar(method='brent')`

## ğŸ—ï¸ **Classe Principal**

### **`CoordinateDescentSolver`**

```python
class CoordinateDescentSolver:
    def __init__(self, block_size=10):
        """
        Inicializa o solver
        
        Args:
            block_size: tamanho do bloco de coordenadas
        """
    
    def solve_problem(self, problem_name, max_iter=1000, tol=1e-6):
        """
        Resolve um problema especÃ­fico
        
        Returns:
            dict: resultados da otimizaÃ§Ã£o
        """
    
    def solve_all_problems(self, max_iter=1000, tol=1e-6):
        """
        Resolve todos os problemas configurados
        """
    
    def print_summary(self):
        """
        Imprime resumo dos resultados
        """
```

## ğŸ“Š **Estrutura dos Resultados**

Cada resultado contÃ©m:
```python
{
    'problem': str,           # Nome do problema
    'success': bool,          # Sucesso da otimizaÃ§Ã£o
    'iterations': int,        # NÃºmero de iteraÃ§Ãµes
    'function_value': float,  # Valor mÃ­nimo encontrado
    'x_value': array,         # Ponto Ã³timo
    'gradient_norm': float,   # Norma do gradiente final
    'message': str,           # Mensagem de status
    'execution_time': float,  # Tempo de execuÃ§Ã£o
    'n_variables': int        # NÃºmero de variÃ¡veis
}
```

## ğŸš€ **Como Usar**

### **Exemplo BÃ¡sico**
```python
from descent_coordinate_algorithm import CoordinateDescentSolver

# Criar solver
solver = CoordinateDescentSolver(block_size=10)

# Resolver um problema especÃ­fico
result = solver.solve_problem('ROSENBROCK', max_iter=1000, tol=1e-6)

# Resolver todos os problemas
solver.solve_all_problems()
solver.print_summary()
```

### **ExecuÃ§Ã£o Completa**
```bash
cd liu_nocedal
python descent_coordinate_algorithm.py
```

## âš™ï¸ **ParÃ¢metros ConfigurÃ¡veis**

| ParÃ¢metro | PadrÃ£o | DescriÃ§Ã£o |
|-----------|--------|-----------|
| `block_size` | 10 | Tamanho do bloco de coordenadas |
| `max_iter` | 1000 | MÃ¡ximo de iteraÃ§Ãµes |
| `tol` | 1e-6 | TolerÃ¢ncia para convergÃªncia |

## ğŸ“ˆ **Vantagens do Algoritmo**

1. **EficiÃªncia**: MinimizaÃ§Ã£o em subespaÃ§os de dimensÃ£o reduzida
2. **ParalelizaÃ§Ã£o**: Coordenadas independentes podem ser processadas em paralelo
3. **MemÃ³ria**: Baixo uso de memÃ³ria comparado a mÃ©todos de segunda ordem
4. **Robustez**: Funciona bem em problemas mal condicionados
5. **Flexibilidade**: Tamanho de bloco adaptÃ¡vel

## ğŸ” **Complexidade Computacional**

- **Por iteraÃ§Ã£o**: O(bÂ·n) onde b Ã© o tamanho do bloco
- **Total**: O(KÂ·bÂ·n) onde K Ã© o nÃºmero de iteraÃ§Ãµes
- **MemÃ³ria**: O(n) - apenas o vetor de variÃ¡veis

## ğŸ“‹ **Problemas Suportados**

O algoritmo resolve todos os problemas da coleÃ§Ã£o Liu e Nocedal:
- Rosenbrock
- Extended Rosenbrock
- Extended Powell
- Freudenthal-Roth
- Engvall
- Trigonometric
- Penalty
- E outros...

## ğŸ“„ **SaÃ­das Geradas**

- **Tabela resumida**: EstatÃ­sticas gerais dos problemas
- **Tabela detalhada**: Valores das variÃ¡veis em mÃºltiplas colunas
- **PDFs**: Gerados automaticamente via LaTeX
- **Logs**: Progresso da otimizaÃ§Ã£o no console
